## MultiGPT3ChoiceMap ##

"""
    MultiGPT3ChoiceMap

An alias for the choicemap associated with [`MultiGPT3Trace`](@ref). 

    choices = MultiGPT3ChoiceMap(outputs::AbstractVector{String})

Constructs a choicemap for the trace of a [`MultiGPT3GenerativeFunction`](@ref).
"""
MultiGPT3ChoiceMap(outputs::AbstractVector{String}) =
    Gen.InternalVectorChoiceMap(GPT3ChoiceMap.(outputs), isempty(outputs))

## MultiGPT3Trace ##

"""
    MultiGPT3Trace

A trace generated by a [`MultiGPT3GenerativeFunction`](@ref). Effectively
represents a vector of [`GPT3Trace`](@ref) subtraces.
"""
struct MultiGPT3Trace{T <: GenerativeFunction} <: Trace
    gen_fn::T
    prompts::Vector{String}
    outputs::Vector{String}
    tokens::Vector{Vector{String}}
    logprobs::Vector{Vector{Float64}}
    scores::Vector{Float64}
    score::Float64
end

get_choices(trace::MultiGPT3Trace) = MultiGPT3ChoiceMap(trace.outputs)
get_args(trace::MultiGPT3Trace) = (trace.prompts,)
get_retval(trace::MultiGPT3Trace) = trace.outputs
get_score(trace::MultiGPT3Trace) = trace.score
get_gen_fn(trace::MultiGPT3Trace) = trace.gen_fn

Base.getindex(trace::MultiGPT3Trace, idx::Int) = trace.outputs[i]

function Base.:(==)(trace1::MultiGPT3Trace, trace2::MultiGPT3Trace)
    return (trace1.gen_fn == trace2.gen_fn &&
            trace1.prompts == trace2.prompts &&
            trace1.outputs == trace2.outputs &&
            trace1.tokens == trace2.tokens &&
            trace1.logprobs == trace2.logprobs &&
            trace1.scores == trace2.scores &&
            trace1.score == trace2.score)
end

## MultiGPT3GenerativeFunction ##

"""
    MultiGPT3GenerativeFunction(;
        model = "text-davinci-002",
        temperature = 1.0,
        max_tokens = 1024,
        stop = nothing,
        api_key_lookup = () -> ENV["OPENAI_API_KEY"],
        organization_lookup = () -> ENV["OPENAI_ORGANIZATION"]
    )

Batched version of [`GPT3GenerativeFunction`](@ref), which requests and 
returns completions for a batch of prompts. Takes in a `Vector` of `String`
valued prompts as an argument. The completion for the `i`th prompt is stored
in the `i => $OUTPUT_ADDR` address of the resulting trace.
"""
@kwdef struct MultiGPT3GenerativeFunction <: GenerativeFunction{String,MultiGPT3Trace}
    model::String = "text-davinci-002"
    temperature::Float64 = 1.0
    max_tokens::Int = 1024
    stop::Union{String,Nothing} = nothing
    n_stop::Int = isnothing(stop) ? 1 : length(tokenize(stop))
    api_key_lookup::Function = lookup_openai_api_key
    organization_lookup::Function = lookup_openai_organization
end

"""
    MultiGPT3GF

An alias for [`MultiGPT3GenerativeFunction`](@ref).
"""
const MultiGPT3GF = MultiGPT3GenerativeFunction

"""
    (gpt3::MultiGPT3GenerativeFunction)(prompts::Vector{String})

Untraced execution of a [`MultiGPT3GenerativeFunction`]. Calls GPT-3 with
a batch of prompts, and returns the resulting completions.
"""
(gen_fn::MultiGPT3GenerativeFunction)(prompts::Vector{String}) =
    get_retval(simulate(gen_fn, (prompts,)))

(gen_fn::MultiGPT3GenerativeFunction)(prompt::String, n::Int) =
    gen_fn(fill(prompt, n))
    
function simulate(gen_fn::MultiGPT3GF, args::Tuple{Vector{String}})
    # Extract prompts and initialize arrays
    prompts = args[1]
    n = length(prompts)
    outputs = Vector{String}(undef, n)

    # Prevent <|endoftext|> from being generated when using custom stop tokens
    logit_bias = isnothing(gen_fn.stop) ? nothing : NO_EOT_BIAS
    # Request completions through GPT-3 API
    choices = gpt3_multi_prompt_api_call(
        prompts;
        model=gen_fn.model,
        temperature=gen_fn.temperature,
        max_tokens=gen_fn.max_tokens,
        stop=gen_fn.stop,
        logit_bias=logit_bias,
        api_key=gen_fn.api_key_lookup(),
        organization=gen_fn.organization_lookup()
    )

    # Extract outputs
    for (i, completion) in enumerate(choices)
        outputs[i] = completion.text
    end

    # Evaluate probability of completion by calling `generate`
    trace, _ = generate(gen_fn, args, MultiGPT3ChoiceMap(outputs))

    return trace
end

simulate(gen_fn::MultiGPT3GF, args::Tuple{String, Int}) =
    simulate(gen_fn, (fill(args[1], args[2]),))

function generate(gen_fn::MultiGPT3GF, args::Tuple, constraints::ChoiceMap)
    # Check whether any outputs are constrained
    if isempty(constraints)
        return generate(gen_fn, args, EmptyChoiceMap())
    end

    # Extract prompts and initialize arrays
    prompts = args[1]
    n = length(prompts)
    outputs = Vector{String}(undef, n)
    tokens = Vector{Vector{String}}(undef, n)
    logprobs = Vector{Vector{Float64}}(undef, n)
    scores = Vector{Float64}(undef, n)

    # Extract constrained outputs and construct full texts
    constrained_idxs = Int[]
    unconstrained_idxs = Int[]
    impossible = false
    full_texts = isnothing(gen_fn.stop) ? Vector{Int}[] : String[]
    for i in eachindex(outputs)
        addr = i => OUTPUT_ADDR
        if !has_value(constraints, addr)
            push!(unconstrained_idxs, i)
            continue
        end
        outputs[i] = constraints[addr]
        full_text = construct_full_text(gen_fn.max_tokens,
                                        prompts[i], outputs[i],
                                        gen_fn.stop, gen_fn.n_stop)
        # If nothing is returned, then the constrained output is too long
        if isnothing(full_text)
            scores[i] = -Inf
            impossible = true
            continue
        end
        push!(full_texts, full_text)
        push!(constrained_idxs, i) 
    end

    # Score the full texts for constrained indices
    if !isempty(constrained_idxs)
        choices = gpt3_multi_prompt_api_call(
            full_texts;
            logprobs=0,
            model=gen_fn.model,
            temperature=gen_fn.temperature,
            max_tokens=0,
            echo=true,
            stop=gen_fn.stop,
            logit_bias=isnothing(gen_fn.stop) ? nothing : NO_EOT_BIAS,
            api_key=gen_fn.api_key_lookup(),
            organization=gen_fn.organization_lookup()
        )

        # Extract scores from returned completions
        for (i, completion) in zip(constrained_idxs, choices)
            tokens[i], token_lps =
                extract_tokens_after_prompt(completion, prompts[i])
            logprobs[i] = gen_fn.temperature == 0.0 ?
                zeros(Float64, length(token_lps)) : token_lps ./ gen_fn.temperature
            scores[i] = isempty(logprobs[i]) ? 0.0 : sum(logprobs[i])
        end
    end

    # Sample all unconstrained choices
    if !isempty(unconstrained_idxs)
        partial_trace = simulate(gen_fn, (prompts[unconstrained_idxs],))
        for (k, i) in enumerate(unconstrained_idxs)
            outputs[i] = partial_trace.outputs[k]
            tokens[i] = partial_trace.tokens[k]
            logprobs[i] = partial_trace.logprobs[k]
            scores[i] = partial_trace.scores[k]
        end
    end

    # Construct and return trace and weight
    total_score = isempty(scores) ? 0.0 : sum(scores)
    weight = if impossible
        -Inf
    elseif isempty(constrained_idxs)
        0.0
    else
        sum(scores[constrained_idxs])
    end
    trace = MultiGPT3Trace(gen_fn, prompts, outputs, tokens,
                           logprobs, scores, total_score)
    return trace, weight
end

generate(gen_fn::MultiGPT3GF, args::Tuple{String, Int}, constraints::ChoiceMap) =
    generate(gen_fn, (fill(args[1], args[2]),), constraints)

generate(gen_fn::MultiGPT3GF, args::Tuple, ::EmptyChoiceMap) =
    simulate(gen_fn, args), 0.0

generate(gen_fn::MultiGPT3GF, args::Tuple{String, Int}, ::EmptyChoiceMap) =
    simulate(gen_fn, args), 0.0

function project(trace::MultiGPT3Trace, selection::Selection)
    if isempty(selection) return 0.0 end
    weight = 0.0
    for i in eachindex(trace.prompts)
        addr = i => OUTPUT_ADDR
        weight += addr in selection ? trace.scores[i] : 0.0
    end
    return weight
end

project(trace::MultiGPT3Trace, ::AllSelection) = trace.score

project(trace::MultiGPT3Trace, ::EmptySelection) = 0.0

function update(trace::MultiGPT3Trace, args::Tuple,
                argdiffs::Tuple, constraints::ChoiceMap)
    gen_fn = trace.gen_fn

    # Extract prompts and copy arrays from old trace
    old_prompts, new_prompts = trace.prompts, args[1]
    old_n, new_n = length(old_prompts), length(new_prompts)
    outputs = resize!(copy(trace.outputs), new_n)
    tokens = resize!(copy(trace.tokens), new_n)
    logprobs = resize!(copy(trace.logprobs), new_n)
    scores = resize!(copy(trace.scores), new_n)

    # Extract updated outputs and construct full texts
    updated_idxs = Int[]
    created_idxs = Int[]
    impossible = false
    full_texts = isnothing(gen_fn.stop) ? Vector{Int}[] : String[]
    for i in 1:new_n
        addr = i => OUTPUT_ADDR
        if !has_value(constraints, addr)
            if i > old_n
                push!(created_idxs, i)
                continue
            elseif new_prompts[i] == old_prompts[i]
                continue
            end
        else
            outputs[i] = constraints[addr] # Output was updated
        end
        full_text = construct_full_text(gen_fn.max_tokens,
                                        new_prompts[i], outputs[i],
                                        gen_fn.stop, gen_fn.n_stop)
        # If nothing is returned, then the constrained output is too long
        if isnothing(full_text)
            scores[i] = -Inf
            impossible = true
            continue
        end
        push!(full_texts, full_text)
        push!(updated_idxs, i) 
    end

    # Score the full texts for updated indices
    if !isempty(updated_idxs)
        choices = gpt3_multi_prompt_api_call(
            full_texts;
            logprobs=0,
            model=gen_fn.model,
            temperature=gen_fn.temperature,
            max_tokens=0,
            echo=true,
            stop=gen_fn.stop,
            logit_bias=isnothing(gen_fn.stop) ? nothing : NO_EOT_BIAS,
            api_key=gen_fn.api_key_lookup(),
            organization=gen_fn.organization_lookup()
        )

        # Extract scores from returned completions
        for (i, completion) in zip(updated_idxs, choices)
            tokens[i], token_lps =
                extract_tokens_after_prompt(completion, new_prompts[i])
            logprobs[i] = gen_fn.temperature == 0.0 ?
                zeros(Float64, length(token_lps)) : token_lps ./ gen_fn.temperature
            scores[i] = isempty(logprobs[i]) ? 0.0 : sum(logprobs[i])
        end
    end

    # Sample completions for newly created indices
    if !isempty(created_idxs)
        partial_trace = simulate(gen_fn, (new_prompts[created_idxs],))
        for (k, i) in enumerate(created_idxs)
            outputs[i] = partial_trace.outputs[k]
            tokens[i] = partial_trace.tokens[k]
            logprobs[i] = partial_trace.logprobs[k]
            scores[i] = partial_trace.scores[k]
        end
    end

    # Compute total score and construct new trace
    total_score = isempty(scores) ? 0.0 : sum(scores)
    new_trace = MultiGPT3Trace(gen_fn, new_prompts, outputs, tokens,
                               logprobs, scores, total_score)

    # Compute incremental weight and discarded choices
    weight = 0.0
    discard = choicemap()
    for i in updated_idxs # Contributions from updated indices
        weight += new_trace.scores[i]
        if i <= old_n
            weight -= trace.scores[i]
        end
        if has_value(constraints, i => OUTPUT_ADDR)
            discard[i => OUTPUT_ADDR] = trace.outputs[i]
        end
    end
    for i in (new_n+1):old_n # Contributions from discarded indices
        weight -= trace.scores[i]
        discard[i => OUTPUT_ADDR] = trace.outputs[i]
    end
    weight += impossible ? -Inf : 0

    # Compute diff and return
    retdiff = new_n == old_n && isempty(constraints) && !impossible ?
        NoChange() : UnknownChange()
    return new_trace, weight, retdiff, discard
end

function regenerate(trace::MultiGPT3Trace, args::Tuple,
                    argdiffs::Tuple, selection::Selection)
    gen_fn = trace.gen_fn

    # Extract prompts and copy arrays from old trace
    old_prompts, new_prompts = trace.prompts, args[1]
    old_n, new_n = length(old_prompts), length(new_prompts)
    outputs = resize!(copy(trace.outputs), new_n)
    tokens = resize!(copy(trace.tokens), new_n)
    logprobs = resize!(copy(trace.logprobs), new_n)
    scores = resize!(copy(trace.scores), new_n)

    # Extract selected or updated outputs and construct full texts
    regenerated_idxs = Int[]
    updated_idxs = Int[]
    full_texts = isnothing(gen_fn.stop) ? Vector{Int}[] : String[]
    for i in 1:new_n
        addr = i => OUTPUT_ADDR
        if i > old_n || addr in selection
            push!(regenerated_idxs, i)
            continue
        elseif new_prompts[i] == old_prompts[i]
            continue
        end
        full_text = construct_full_text(gen_fn.max_tokens,
                                        new_prompts[i], outputs[i],
                                        gen_fn.stop, gen_fn.n_stop)
        push!(full_texts, full_text)
        push!(updated_idxs, i) 
    end

    # Score the full texts for updated indices
    if !isempty(updated_idxs)
        choices = gpt3_multi_prompt_api_call(
            full_texts;
            logprobs=0,
            model=gen_fn.model,
            temperature=gen_fn.temperature,
            max_tokens=0,
            echo=true,
            stop=gen_fn.stop,
            logit_bias=isnothing(gen_fn.stop) ? nothing : NO_EOT_BIAS,
            api_key=gen_fn.api_key_lookup(),
            organization=gen_fn.organization_lookup()
        )

        # Extract scores from returned completions
        for (i, completion) in zip(updated_idxs, choices)
            tokens[i], token_lps =
                extract_tokens_after_prompt(completion, new_prompts[i])
            logprobs[i] = gen_fn.temperature == 0.0 ?
                zeros(Float64, length(token_lps)) : token_lps ./ gen_fn.temperature
            scores[i] = isempty(logprobs[i]) ? 0.0 : sum(logprobs[i])
        end
    end

    # Sample completions for selected and newly created indices
    if !isempty(regenerated_idxs)
        partial_trace = simulate(gen_fn, (new_prompts[regenerated_idxs],))
        for (k, i) in enumerate(regenerated_idxs)
            outputs[i] = partial_trace.outputs[k]
            tokens[i] = partial_trace.tokens[k]
            logprobs[i] = partial_trace.logprobs[k]
            scores[i] = partial_trace.scores[k]
        end
    end

    # Compute total score and construct new trace
    total_score = isempty(scores) ? 0.0 : sum(scores)
    new_trace = MultiGPT3Trace(gen_fn, new_prompts, outputs, tokens,
                               logprobs, scores, total_score)

    # Compute incremental weight 
    weight = 0.0
    for i in updated_idxs # Contributions from updated indices
        weight += new_trace.scores[i] - trace.scores[i]
    end

    # Compute diff and return
    retdiff = new_n == old_n && isempty(regenerated_idxs) ?
        NoChange() : UnknownChange()
    return new_trace, weight, retdiff
end
